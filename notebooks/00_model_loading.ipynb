{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvNext SOTA Model - Setup & Baseline\n",
    "\n",
    "**Project**: Probing ConvNext with Hard Examples  \n",
    "**Goal**: Load ConvNext-Base, establish baseline accuracy, and prepare for hard example generation  \n",
    "**Week**: 1 (Setup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Import all required libraries and verify versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Timm for ConvNext\n",
    "import timm\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Version info\n",
    "print(\"=\" * 60)\n",
    "print(\"ENVIRONMENT VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Torchvision Version: {torchvision.__version__}\")\n",
    "print(f\"Timm Version: {timm.__version__}\")\n",
    "print(f\"NumPy Version: {np.__version__}\")\n",
    "print(f\"Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load ConvNext-Base Model\n",
    "\n",
    "Load pretrained ConvNext-Base from ImageNet-1K using timm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = 'convnext_base'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "model = timm.create_model(MODEL_NAME, pretrained=True, num_classes=1000)\n",
    "model = model.to(DEVICE)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Get model info\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Data Transforms\n",
    "\n",
    "Set up standard ImageNet preprocessing for ConvNext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageNet normalization values\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "# Inverse transform for visualization\n",
    "inv_normalize = transforms.Compose([\n",
    "    transforms.Normalize(\n",
    "        mean=[-m/s for m, s in zip(IMAGENET_MEAN, IMAGENET_STD)],\n",
    "        std=[1/s for s in IMAGENET_STD]\n",
    "    ),\n",
    "    transforms.ToPILImage()\n",
    "])\n",
    "\n",
    "print(\"‚úì Transform pipeline configured\")\n",
    "print(f\"  - Input size: {IMAGE_SIZE}x{IMAGE_SIZE}\")\n",
    "print(f\"  - ImageNet normalization applied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Model Inference\n",
    "\n",
    "Run a simple inference test on a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference with a dummy image\n",
    "test_input = torch.randn(1, 3, IMAGE_SIZE, IMAGE_SIZE).to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(test_input)\n",
    "\n",
    "print(f\"‚úì Model inference test passed\")\n",
    "print(f\"  - Input shape: {test_input.shape}\")\n",
    "print(f\"  - Output shape: {output.shape}\")\n",
    "print(f\"  - Output classes: {output.shape[1]}\")\n",
    "\n",
    "# Get top-5 predictions for dummy image\n",
    "probabilities = torch.softmax(output, dim=1)\n",
    "top5_prob, top5_idx = torch.topk(probabilities, 5)\n",
    "print(f\"\\n  - Top-5 prediction scores: {top5_prob[0].cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load ImageNet Labels\n",
    "\n",
    "Load ImageNet class labels for result interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download ImageNet labels if not available\n",
    "import urllib.request\n",
    "\n",
    "LABELS_URL = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
    "LABELS_FILE = \"../data/imagenet_classes.txt\"\n",
    "\n",
    "# Create data directory\n",
    "Path(\"../data\").mkdir(exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Try to load existing labels\n",
    "    with open(LABELS_FILE, 'r') as f:\n",
    "        imagenet_labels = [line.strip() for line in f.readlines()]\n",
    "    print(f\"‚úì Loaded ImageNet labels from file ({len(imagenet_labels)} classes)\")\nexcept FileNotFoundError:\n",
    "    print(\"Downloading ImageNet labels...\")\n",
    "    urllib.request.urlretrieve(LABELS_URL, LABELS_FILE)\n",
    "    with open(LABELS_FILE, 'r') as f:\n",
    "        imagenet_labels = [line.strip() for line in f.readlines()]\n",
    "    print(f\"‚úì Downloaded ImageNet labels ({len(imagenet_labels)} classes)\")\n",
    "\n",
    "print(f\"\\n  Sample labels:\")\n",
    "for i in range(0, 5):\n",
    "    print(f\"    {i}: {imagenet_labels[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Helper Functions\n",
    "\n",
    "Define utility functions for model evaluation and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(image_tensor, top_k=5):\n",
    "    \"\"\"Get model prediction for a single image.\n",
    "    \n",
    "    Args:\n",
    "        image_tensor: Preprocessed image tensor (1, 3, H, W)\n",
    "        top_k: Number of top predictions to return\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains top-k predictions with indices and scores\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor.unsqueeze(0).to(DEVICE))\n",
    "        probabilities = torch.softmax(output, dim=1)\n",
    "        top_probs, top_indices = torch.topk(probabilities, top_k)\n",
    "    \n",
    "    predictions = []\n",
    "    for prob, idx in zip(top_probs[0].cpu().numpy(), top_indices[0].cpu().numpy()):\n",
    "        predictions.append({\n",
    "            'class_id': int(idx),\n",
    "            'class_name': imagenet_labels[idx],\n",
    "            'confidence': float(prob)\n",
    "        })\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def visualize_predictions(image_tensor, predictions, title=\"Prediction\"):\n",
    "    \"\"\"Visualize image with top predictions.\n",
    "    \n",
    "    Args:\n",
    "        image_tensor: Preprocessed image tensor\n",
    "        predictions: Output from get_prediction()\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    # Convert back to PIL for display\n",
    "    pil_image = inv_normalize(image_tensor.cpu())\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Show image\n",
    "    ax1.imshow(pil_image)\n",
    "    ax1.set_title(title)\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Show predictions\n",
    "    class_names = [p['class_name'] for p in predictions]\n",
    "    confidences = [p['confidence'] for p in predictions]\n",
    "    \n",
    "    ax2.barh(range(len(predictions)), confidences)\n",
    "    ax2.set_yticks(range(len(predictions)))\n",
    "    ax2.set_yticklabels(class_names)\n",
    "    ax2.set_xlabel('Confidence')\n",
    "    ax2.set_title('Top-5 Predictions')\n",
    "    ax2.set_xlim([0, 1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "print(\"‚úì Helper functions defined:\")\n",
    "print(\"  - get_prediction(): Get model predictions\")\n",
    "print(\"  - visualize_predictions(): Visualize predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Project Status\n",
    "\n",
    "Summary of setup and next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PROJECT SETUP COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚úì COMPLETED:\")\n",
    "print(\"  1. Loaded ConvNext-Base (ImageNet pretrained)\")\n",
    "print(\"  2. Configured preprocessing pipeline\")\n",
    "print(\"  3. Verified model inference\")\n",
    "print(\"  4. Loaded ImageNet class labels\")\n",
    "print(\"  5. Created helper functions\")\n",
    "print(\"\\n‚è≠Ô∏è  NEXT (Week 2):\")\n",
    "print(\"  1. Load ImageNet validation set subset\")\n",
    "print(\"  2. Establish baseline accuracy metrics\")\n",
    "print(\"  3. Implement FGSM adversarial attack\")\n",
    "print(\"  4. Generate initial hard examples\")\n",
    "print(\"  5. Document failure patterns\")\n",
    "print(\"\\nüí° ATTACK STRATEGIES TO IMPLEMENT:\")\n",
    "print(\"  - Adversarial: FGSM, PGD, Auto-Attack, C&W\")\n",
    "print(\"  - OOD Detection: Distribution shifts, synthetic images\")\n",
    "print(\"  - Corner Cases: Texture-only, minimal objects, extreme lighting\")\n",
    "print(\"  - Domain Adaptation: Style transfer, cross-dataset mismatch\")\n",
    "print(\"  - Edge Cases: Multi-object, fine-grained, similar classes\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
