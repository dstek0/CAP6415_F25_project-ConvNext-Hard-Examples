{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvNext SOTA Model - Setup & Baseline\n",
    "\n",
    "**Project**: Probing ConvNext with Hard Examples  \n",
    "**Goal**: Load ConvNext-Base, establish baseline accuracy, and prepare for hard example generation  \n",
    "**Week**: 1 (Setup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Import all required libraries and verify versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ENVIRONMENT VERIFICATION\n",
      "============================================================\n",
      "PyTorch Version: 2.9.0+cu126\n",
      "CUDA Available: False\n",
      "Torchvision Version: 0.24.0+cu126\n",
      "Timm Version: 1.0.22\n",
      "NumPy Version: 2.0.2\n",
      "Device: cpu\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Timm for ConvNext\n",
    "import timm\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Version info\n",
    "print(\"=\" * 60)\n",
    "print(\"ENVIRONMENT VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Torchvision Version: {torchvision.__version__}\")\n",
    "print(f\"Timm Version: {timm.__version__}\")\n",
    "print(f\"NumPy Version: {np.__version__}\")\n",
    "print(f\"Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load ConvNext-Base Model\n",
    "\n",
    "Load pretrained ConvNext-Base from ImageNet-1K using timm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading convnext_base...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06e2234360347638f46f3c765017f4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/354M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: convnext_base\n",
      "Parameters: 88,591,464\n",
      "Trainable: 88,591,464\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = 'convnext_base'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "model = timm.create_model(MODEL_NAME, pretrained=True, num_classes=1000)\n",
    "model = model.to(DEVICE)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Get model info\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Data Transforms\n",
    "\n",
    "Set up standard ImageNet preprocessing for ConvNext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Transform pipeline configured\n",
      "  - Input size: 224x224\n",
      "  - ImageNet normalization applied\n"
     ]
    }
   ],
   "source": [
    "# ImageNet normalization values\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "# Inverse transform for visualization\n",
    "inv_normalize = transforms.Compose([\n",
    "    transforms.Normalize(\n",
    "        mean=[-m/s for m, s in zip(IMAGENET_MEAN, IMAGENET_STD)],\n",
    "        std=[1/s for s in IMAGENET_STD]\n",
    "    ),\n",
    "    transforms.ToPILImage()\n",
    "])\n",
    "\n",
    "print(\"âœ“ Transform pipeline configured\")\n",
    "print(f\"  - Input size: {IMAGE_SIZE}x{IMAGE_SIZE}\")\n",
    "print(f\"  - ImageNet normalization applied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Model Inference\n",
    "\n",
    "Run a simple inference test on a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model inference test passed\n",
      "  - Input shape: torch.Size([1, 3, 224, 224])\n",
      "  - Output shape: torch.Size([1, 1000])\n",
      "  - Output classes: 1000\n",
      "\n",
      "  - Top-5 prediction scores: [0.00212155 0.00195865 0.00191272 0.00182453 0.00177232]\n"
     ]
    }
   ],
   "source": [
    "# Test inference with a dummy image\n",
    "test_input = torch.randn(1, 3, IMAGE_SIZE, IMAGE_SIZE).to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(test_input)\n",
    "\n",
    "print(f\"âœ“ Model inference test passed\")\n",
    "print(f\"  - Input shape: {test_input.shape}\")\n",
    "print(f\"  - Output shape: {output.shape}\")\n",
    "print(f\"  - Output classes: {output.shape[1]}\")\n",
    "\n",
    "# Get top-5 predictions for dummy image\n",
    "probabilities = torch.softmax(output, dim=1)\n",
    "top5_prob, top5_idx = torch.topk(probabilities, 5)\n",
    "print(f\"\\n  - Top-5 prediction scores: {top5_prob[0].cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load ImageNet Labels\n",
    "\n",
    "Load ImageNet class labels for result interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ImageNet labels...\n",
      "âœ“ Downloaded ImageNet labels (1000 classes)\n",
      "\n",
      "  Sample labels:\n",
      "    0: tench\n",
      "    1: goldfish\n",
      "    2: great white shark\n",
      "    3: tiger shark\n",
      "    4: hammerhead\n"
     ]
    }
   ],
   "source": [
    "# Download ImageNet labels if not available\n",
    "import urllib.request\n",
    "\n",
    "LABELS_URL = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
    "LABELS_FILE = \"../data/imagenet_classes.txt\"\n",
    "\n",
    "# Create data directory\n",
    "Path(\"../data\").mkdir(exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Try to load existing labels\n",
    "    with open(LABELS_FILE, 'r') as f:\n",
    "        imagenet_labels = [line.strip() for line in f.readlines()]\n",
    "    print(f\"âœ“ Loaded ImageNet labels from file ({len(imagenet_labels)} classes)\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Downloading ImageNet labels...\")\n",
    "    urllib.request.urlretrieve(LABELS_URL, LABELS_FILE)\n",
    "    with open(LABELS_FILE, 'r') as f:\n",
    "        imagenet_labels = [line.strip() for line in f.readlines()]\n",
    "    print(f\"âœ“ Downloaded ImageNet labels ({len(imagenet_labels)} classes)\")\n",
    "\n",
    "print(f\"\\n  Sample labels:\")\n",
    "for i in range(0, 5):\n",
    "    print(f\"    {i}: {imagenet_labels[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Helper Functions\n",
    "\n",
    "Define utility functions for model evaluation and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Helper functions defined:\n",
      "  - get_prediction(): Get model predictions\n",
      "  - visualize_predictions(): Visualize predictions\n"
     ]
    }
   ],
   "source": [
    "def get_prediction(image_tensor, top_k=5):\n",
    "    \"\"\"Get model prediction for a single image.\n",
    "    \n",
    "    Args:\n",
    "        image_tensor: Preprocessed image tensor (1, 3, H, W)\n",
    "        top_k: Number of top predictions to return\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains top-k predictions with indices and scores\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor.unsqueeze(0).to(DEVICE))\n",
    "        probabilities = torch.softmax(output, dim=1)\n",
    "        top_probs, top_indices = torch.topk(probabilities, top_k)\n",
    "    \n",
    "    predictions = []\n",
    "    for prob, idx in zip(top_probs[0].cpu().numpy(), top_indices[0].cpu().numpy()):\n",
    "        predictions.append({\n",
    "            'class_id': int(idx),\n",
    "            'class_name': imagenet_labels[idx],\n",
    "            'confidence': float(prob)\n",
    "        })\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def visualize_predictions(image_tensor, predictions, title=\"Prediction\"):\n",
    "    \"\"\"Visualize image with top predictions.\n",
    "    \n",
    "    Args:\n",
    "        image_tensor: Preprocessed image tensor\n",
    "        predictions: Output from get_prediction()\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    # Convert back to PIL for display\n",
    "    pil_image = inv_normalize(image_tensor.cpu())\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Show image\n",
    "    ax1.imshow(pil_image)\n",
    "    ax1.set_title(title)\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Show predictions\n",
    "    class_names = [p['class_name'] for p in predictions]\n",
    "    confidences = [p['confidence'] for p in predictions]\n",
    "    \n",
    "    ax2.barh(range(len(predictions)), confidences)\n",
    "    ax2.set_yticks(range(len(predictions)))\n",
    "    ax2.set_yticklabels(class_names)\n",
    "    ax2.set_xlabel('Confidence')\n",
    "    ax2.set_title('Top-5 Predictions')\n",
    "    ax2.set_xlim([0, 1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "print(\"âœ“ Helper functions defined:\")\n",
    "print(\"  - get_prediction(): Get model predictions\")\n",
    "print(\"  - visualize_predictions(): Visualize predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Project Status\n",
    "\n",
    "Summary of setup and next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PROJECT SETUP COMPLETE\n",
      "======================================================================\n",
      "\n",
      "âœ“ COMPLETED:\n",
      "  1. Loaded ConvNext-Base (ImageNet pretrained)\n",
      "  2. Configured preprocessing pipeline\n",
      "  3. Verified model inference\n",
      "  4. Loaded ImageNet class labels\n",
      "  5. Created helper functions\n",
      "\n",
      "â­ï¸  NEXT (Week 2):\n",
      "  1. Load ImageNet validation set subset\n",
      "  2. Establish baseline accuracy metrics\n",
      "  3. Implement FGSM adversarial attack\n",
      "  4. Generate initial hard examples\n",
      "  5. Document failure patterns\n",
      "\n",
      "ðŸ’¡ ATTACK STRATEGIES TO IMPLEMENT:\n",
      "  - Adversarial: FGSM, PGD, Auto-Attack, C&W\n",
      "  - OOD Detection: Distribution shifts, synthetic images\n",
      "  - Corner Cases: Texture-only, minimal objects, extreme lighting\n",
      "  - Domain Adaptation: Style transfer, cross-dataset mismatch\n",
      "  - Edge Cases: Multi-object, fine-grained, similar classes\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PROJECT SETUP COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nâœ“ COMPLETED:\")\n",
    "print(\"  1. Loaded ConvNext-Base (ImageNet pretrained)\")\n",
    "print(\"  2. Configured preprocessing pipeline\")\n",
    "print(\"  3. Verified model inference\")\n",
    "print(\"  4. Loaded ImageNet class labels\")\n",
    "print(\"  5. Created helper functions\")\n",
    "print(\"\\nâ­ï¸  NEXT (Week 2):\")\n",
    "print(\"  1. Load ImageNet validation set subset\")\n",
    "print(\"  2. Establish baseline accuracy metrics\")\n",
    "print(\"  3. Implement FGSM adversarial attack\")\n",
    "print(\"  4. Generate initial hard examples\")\n",
    "print(\"  5. Document failure patterns\")\n",
    "print(\"\\nðŸ’¡ ATTACK STRATEGIES TO IMPLEMENT:\")\n",
    "print(\"  - Adversarial: FGSM, PGD, Auto-Attack, C&W\")\n",
    "print(\"  - OOD Detection: Distribution shifts, synthetic images\")\n",
    "print(\"  - Corner Cases: Texture-only, minimal objects, extreme lighting\")\n",
    "print(\"  - Domain Adaptation: Style transfer, cross-dataset mismatch\")\n",
    "print(\"  - Edge Cases: Multi-object, fine-grained, similar classes\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
