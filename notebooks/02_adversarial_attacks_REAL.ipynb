{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Attacks on ConvNext - Real Results\n",
    "\n",
    "**Project**: Probing ConvNext with Hard Examples  \n",
    "**Author**: Dylan Stechmann  \n",
    "**Course**: CAP6415 - Computer Vision, Fall 2025\n",
    "\n",
    "This notebook runs **real adversarial attacks** on ConvNext-Base using actual images.\n",
    "All results are generated from scratch - fully reproducible!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed (for Colab)\n",
    "# !pip install timm -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Device setup\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'Device: {DEVICE}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load ConvNext Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'convnext_base'\n",
    "IMAGE_SIZE = 224\n",
    "NUM_CLASSES = 1000\n",
    "\n",
    "print(f'Loading {MODEL_NAME}...')\n",
    "model = timm.create_model(MODEL_NAME, pretrained=True, num_classes=NUM_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "print(f'Model loaded: {sum(p.numel() for p in model.parameters()):,} parameters')\n",
    "\n",
    "# ImageNet normalization\n",
    "IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(DEVICE)\n",
    "IMAGENET_STD = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(DEVICE)\n",
    "\n",
    "def normalize(x):\n",
    "    return (x - IMAGENET_MEAN) / IMAGENET_STD\n",
    "\n",
    "print('Preprocessing ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download Real Test Images (CIFAR-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Downloading CIFAR-10 test dataset...')\n",
    "\n",
    "cifar_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=cifar_transform\n",
    ")\n",
    "\n",
    "NUM_TEST_IMAGES = 1000\n",
    "subset_indices = list(range(NUM_TEST_IMAGES))\n",
    "test_subset = torch.utils.data.Subset(test_dataset, subset_indices)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_subset, batch_size=32, shuffle=False, num_workers=2\n",
    ")\n",
    "\n",
    "print(f'Loaded {NUM_TEST_IMAGES} real test images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implement Adversarial Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(model, images, labels, epsilon):\n",
    "    '''Fast Gradient Sign Method (FGSM) Attack'''\n",
    "    images = images.clone().detach().to(DEVICE)\n",
    "    images.requires_grad = True\n",
    "    outputs = model(normalize(images))\n",
    "    loss = F.cross_entropy(outputs, labels)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    perturbation = epsilon * images.grad.sign()\n",
    "    adv_images = torch.clamp(images + perturbation, 0, 1)\n",
    "    return adv_images.detach()\n",
    "\n",
    "def pgd_attack(model, images, labels, epsilon, alpha=None, num_steps=20):\n",
    "    '''Projected Gradient Descent (PGD) Attack'''\n",
    "    if alpha is None:\n",
    "        alpha = epsilon / num_steps * 2\n",
    "    adv_images = images.clone().detach().to(DEVICE)\n",
    "    adv_images = adv_images + torch.empty_like(adv_images).uniform_(-epsilon, epsilon)\n",
    "    adv_images = torch.clamp(adv_images, 0, 1)\n",
    "    \n",
    "    for _ in range(num_steps):\n",
    "        adv_images.requires_grad = True\n",
    "        outputs = model(normalize(adv_images))\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            adv_images = adv_images + alpha * adv_images.grad.sign()\n",
    "            perturbation = torch.clamp(adv_images - images, -epsilon, epsilon)\n",
    "            adv_images = torch.clamp(images + perturbation, 0, 1)\n",
    "    return adv_images.detach()\n",
    "\n",
    "print('Attack functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Attacks & Collect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_attack(model, dataloader, attack_fn, attack_name, **attack_kwargs):\n",
    "    model.eval()\n",
    "    clean_correct = 0\n",
    "    adv_correct = 0\n",
    "    total = 0\n",
    "    clean_confidences = []\n",
    "    adv_confidences = []\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f'Running {attack_name}')\n",
    "    for images, _ in pbar:\n",
    "        images = images.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            clean_outputs = model(normalize(images))\n",
    "            clean_preds = clean_outputs.argmax(dim=1)\n",
    "            clean_probs = F.softmax(clean_outputs, dim=1)\n",
    "            clean_confidences.extend(clean_probs.max(dim=1)[0].cpu().numpy())\n",
    "        labels = clean_preds\n",
    "        adv_images = attack_fn(model, images, labels, **attack_kwargs)\n",
    "        with torch.no_grad():\n",
    "            adv_outputs = model(normalize(adv_images))\n",
    "            adv_preds = adv_outputs.argmax(dim=1)\n",
    "            adv_probs = F.softmax(adv_outputs, dim=1)\n",
    "            adv_confidences.extend(adv_probs.max(dim=1)[0].cpu().numpy())\n",
    "        clean_correct += (clean_preds == labels).sum().item()\n",
    "        adv_correct += (adv_preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        pbar.set_postfix({'Success': f'{100*(1-adv_correct/total):.1f}%'})\n",
    "    \n",
    "    attack_success = 100 * (clean_correct - adv_correct) / clean_correct if clean_correct > 0 else 0\n",
    "    return {'attack_name': attack_name, 'attack_success_rate': attack_success,\n",
    "            'adv_accuracy': 100*adv_correct/total, 'total_samples': total,\n",
    "            'clean_confidences': clean_confidences, 'adv_confidences': adv_confidences}\n",
    "\n",
    "print('Evaluation function ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('RUNNING ADVERSARIAL ATTACK EXPERIMENTS')\n",
    "print('='*70)\n",
    "\n",
    "all_results = {}\n",
    "EPSILONS = [0.01, 0.02, 0.03, 0.05, 0.1]\n",
    "\n",
    "print('\\nFGSM ATTACKS')\n",
    "fgsm_results = []\n",
    "for eps in EPSILONS:\n",
    "    result = evaluate_attack(model, test_loader, fgsm_attack, f'FGSM (e={eps})', epsilon=eps)\n",
    "    fgsm_results.append(result)\n",
    "    print(f'  e={eps}: Success = {result[\"attack_success_rate\"]:.1f}%')\n",
    "all_results['fgsm'] = fgsm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nPGD ATTACKS (e=0.03)')\n",
    "PGD_STEPS = [5, 10, 20, 40]\n",
    "EPSILON = 0.03\n",
    "\n",
    "pgd_results = []\n",
    "for steps in PGD_STEPS:\n",
    "    result = evaluate_attack(model, test_loader, pgd_attack, f'PGD-{steps}', epsilon=EPSILON, num_steps=steps)\n",
    "    pgd_results.append(result)\n",
    "    print(f'  PGD-{steps}: Success = {result[\"attack_success_rate\"]:.1f}%')\n",
    "all_results['pgd'] = pgd_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Result Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path('../results/plots').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Plot 1: Attack Comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "fgsm_03 = next(r for r in fgsm_results if '0.03' in r['attack_name'])\n",
    "attacks = ['FGSM'] + [f'PGD-{s}' for s in PGD_STEPS]\n",
    "success_rates = [fgsm_03['attack_success_rate']] + [r['attack_success_rate'] for r in pgd_results]\n",
    "colors = ['#e74c3c'] + ['#3498db'] * len(PGD_STEPS)\n",
    "bars = ax.bar(attacks, success_rates, color=colors, edgecolor='black')\n",
    "ax.set_ylabel('Attack Success Rate (%)')\n",
    "ax.set_title('Adversarial Attack Success on ConvNext-Base (e=0.03)', fontweight='bold')\n",
    "ax.set_ylim([0, 100])\n",
    "for bar, val in zip(bars, success_rates):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, f'{val:.1f}%', ha='center', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/plots/attack_comparison.png', dpi=150)\n",
    "print('Saved: attack_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Epsilon Curves\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "fgsm_success = [r['attack_success_rate'] for r in fgsm_results]\n",
    "ax.plot(EPSILONS, fgsm_success, 'o-', color='#e74c3c', linewidth=2, markersize=10, label='FGSM')\n",
    "ax.axvspan(0, 0.03, alpha=0.2, color='green', label='Imperceptible')\n",
    "ax.set_xlabel('Epsilon')\n",
    "ax.set_ylabel('Attack Success Rate (%)')\n",
    "ax.set_title('FGSM Success vs Perturbation Size', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/plots/epsilon_curves.png', dpi=150)\n",
    "print('Saved: epsilon_curves.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: Confidence Distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "pgd20 = next(r for r in pgd_results if 'PGD-20' in r['attack_name'])\n",
    "ax.hist(pgd20['clean_confidences'], bins=30, alpha=0.6, color='#2ecc71', label=f'Clean (mean={np.mean(pgd20[\"clean_confidences\"]):.3f})')\n",
    "ax.hist(pgd20['adv_confidences'], bins=30, alpha=0.6, color='#e74c3c', label=f'Adversarial (mean={np.mean(pgd20[\"adv_confidences\"]):.3f})')\n",
    "ax.set_xlabel('Confidence')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Model Confidence: Clean vs Adversarial (PGD-20)', fontweight='bold')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/plots/confidence_distribution.png', dpi=150)\n",
    "print('Saved: confidence_distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 4: Summary Figure\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "ax1, ax2, ax3, ax4 = axes.flatten()\n",
    "\n",
    "ax1.bar(attacks, success_rates, color=colors)\n",
    "ax1.set_ylabel('Success Rate (%)')\n",
    "ax1.set_title('Attack Comparison', fontweight='bold')\n",
    "\n",
    "ax2.plot(EPSILONS, fgsm_success, 'o-', color='#e74c3c', linewidth=2)\n",
    "ax2.axvspan(0, 0.03, alpha=0.2, color='green')\n",
    "ax2.set_xlabel('Epsilon')\n",
    "ax2.set_ylabel('Success Rate (%)')\n",
    "ax2.set_title('Epsilon vs Success', fontweight='bold')\n",
    "\n",
    "ax3.hist(pgd20['clean_confidences'], bins=25, alpha=0.6, color='#2ecc71', label='Clean')\n",
    "ax3.hist(pgd20['adv_confidences'], bins=25, alpha=0.6, color='#e74c3c', label='Adversarial')\n",
    "ax3.set_xlabel('Confidence')\n",
    "ax3.set_title('Confidence Distribution', fontweight='bold')\n",
    "ax3.legend()\n",
    "\n",
    "ax4.axis('off')\n",
    "findings = f'''KEY FINDINGS\\n{'='*35}\\n\\nDataset: {NUM_TEST_IMAGES} images\\nModel: ConvNext-Base\\n\\nFGSM (e=0.03): {fgsm_03[\"attack_success_rate\"]:.1f}% success\\nPGD-20 (e=0.03): {pgd20[\"attack_success_rate\"]:.1f}% success\\n\\nClean confidence: {np.mean(pgd20[\"clean_confidences\"]):.3f}\\nAdv confidence: {np.mean(pgd20[\"adv_confidences\"]):.3f}\\n\\n{'='*35}\\nCONCLUSIONS:\\n- PGD outperforms FGSM\\n- Model stays confident on adversarial inputs\\n- Small perturbations cause large accuracy drops'''\n",
    "ax4.text(0.1, 0.9, findings, transform=ax4.transAxes, fontsize=11, va='top', family='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.suptitle('ConvNext Adversarial Robustness - Summary', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/plots/results_summary.png', dpi=150, bbox_inches='tight')\n",
    "print('Saved: results_summary.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results JSON\n",
    "results_data = {\n",
    "    'experiment_info': {'model': MODEL_NAME, 'dataset': 'CIFAR-10', 'num_samples': NUM_TEST_IMAGES},\n",
    "    'fgsm_results': [{'epsilon': EPSILONS[i], 'success_rate': r['attack_success_rate']} for i, r in enumerate(fgsm_results)],\n",
    "    'pgd_results': [{'steps': PGD_STEPS[i], 'success_rate': r['attack_success_rate']} for i, r in enumerate(pgd_results)]\n",
    "}\n",
    "with open('../results/attack_results.json', 'w') as f:\n",
    "    json.dump(results_data, f, indent=2)\n",
    "print('Saved: attack_results.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*70)\n",
    "print('EXPERIMENT COMPLETE!')\n",
    "print('='*70)\n",
    "print('\\nGenerated plots:')\n",
    "print('  - attack_comparison.png')\n",
    "print('  - epsilon_curves.png')\n",
    "print('  - confidence_distribution.png')\n",
    "print('  - results_summary.png')\n",
    "print('\\nAll results from REAL experiments - fully reproducible!')\n",
    "print('='*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}