{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Testing - ConvNext on ImageNet\n",
    "\n",
    "**Project**: Probing ConvNext with Hard Examples  \n",
    "**Week**: 2  \n",
    "**Goal**: Establish baseline accuracy metrics on clean ImageNet validation images\n",
    "\n",
    "This notebook:\n",
    "1. Loads a subset of ImageNet validation data\n",
    "2. Tests ConvNext-Base on clean images\n",
    "3. Records accuracy metrics (top-1, top-5)\n",
    "4. Identifies correctly classified images for later adversarial attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = 'convnext_base'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Load model\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "model = timm.create_model(MODEL_NAME, pretrained=True, num_classes=1000)\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "print(f\"✓ Model loaded with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "# ImageNet preprocessing\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "print(\"✓ Preprocessing pipeline ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ImageNet Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or download ImageNet class labels\n",
    "import urllib.request\n",
    "\n",
    "LABELS_URL = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
    "LABELS_FILE = \"../data/imagenet_classes.txt\"\n",
    "\n",
    "Path(\"../data\").mkdir(exist_ok=True)\n",
    "\n",
    "try:\n",
    "    with open(LABELS_FILE, 'r') as f:\n",
    "        imagenet_labels = [line.strip() for line in f.readlines()]\n",
    "    print(f\"✓ Loaded {len(imagenet_labels)} ImageNet class labels\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Downloading ImageNet labels...\")\n",
    "    urllib.request.urlretrieve(LABELS_URL, LABELS_FILE)\n",
    "    with open(LABELS_FILE, 'r') as f:\n",
    "        imagenet_labels = [line.strip() for line in f.readlines()]\n",
    "    print(f\"✓ Downloaded {len(imagenet_labels)} labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ImageNet Validation Dataset\n",
    "\n",
    "**Note**: You'll need to download ImageNet validation set and place it in `../data/imagenet/val/`  \n",
    "Alternatively, use a smaller subset for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update this path to your ImageNet validation data location\n",
    "IMAGENET_VAL_PATH = \"../data/imagenet/val/\"\n",
    "\n",
    "# Load dataset (if available)\n",
    "if Path(IMAGENET_VAL_PATH).exists():\n",
    "    val_dataset = datasets.ImageFolder(IMAGENET_VAL_PATH, transform=preprocess)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=4\n",
    "    )\n",
    "    print(f\"✓ Loaded validation dataset: {len(val_dataset)} images\")\n",
    "else:\n",
    "    print(\"⚠ ImageNet validation data not found at:\", IMAGENET_VAL_PATH)\n",
    "    print(\"  For testing, you can use a smaller sample dataset or downloaded images.\")\n",
    "    val_dataset = None\n",
    "    val_loader = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Accuracy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device, num_batches=None):\n",
    "    \"\"\"\n",
    "    Evaluate model accuracy on validation data.\n",
    "    \n",
    "    Args:\n",
    "        model: Pretrained model\n",
    "        dataloader: DataLoader for validation set\n",
    "        device: torch.device\n",
    "        num_batches: Limit evaluation to N batches (for speed)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Accuracy metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    correct_top1 = 0\n",
    "    correct_top5 = 0\n",
    "    total = 0\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc=\"Evaluating\")\n",
    "        for batch_idx, (images, labels) in enumerate(pbar):\n",
    "            if num_batches and batch_idx >= num_batches:\n",
    "                break\n",
    "            \n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Top-1 accuracy\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct_top1 += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            # Top-5 accuracy\n",
    "            _, top5_pred = outputs.topk(5, dim=1)\n",
    "            correct_top5 += sum([labels[i] in top5_pred[i] for i in range(len(labels))])\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            \n",
    "            # Store for confusion matrix\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'Top-1': f\"{100.*correct_top1/total:.2f}%\",\n",
    "                'Top-5': f\"{100.*correct_top5/total:.2f}%\"\n",
    "            })\n",
    "    \n",
    "    return {\n",
    "        'top1_accuracy': 100. * correct_top1 / total,\n",
    "        'top5_accuracy': 100. * correct_top5 / total,\n",
    "        'total_samples': total,\n",
    "        'predictions': all_predictions,\n",
    "        'labels': all_labels\n",
    "    }\n",
    "\n",
    "# Run evaluation (if dataset is available)\n",
    "if val_loader:\n",
    "    print(\"\\nEvaluating baseline accuracy...\")\n",
    "    # Evaluate on first 1000 images for speed\n",
    "    results = evaluate_model(model, val_loader, DEVICE, num_batches=1000//BATCH_SIZE)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BASELINE ACCURACY RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Top-1 Accuracy: {results['top1_accuracy']:.2f}%\")\n",
    "    print(f\"Top-5 Accuracy: {results['top5_accuracy']:.2f}%\")\n",
    "    print(f\"Total Samples:  {results['total_samples']}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Save results\n",
    "    Path(\"../results\").mkdir(exist_ok=True)\n",
    "    with open(\"../results/baseline_accuracy.json\", 'w') as f:\n",
    "        json.dump({\n",
    "            'top1_accuracy': results['top1_accuracy'],\n",
    "            'top5_accuracy': results['top5_accuracy'],\n",
    "            'total_samples': results['total_samples']\n",
    "        }, f, indent=2)\n",
    "    print(\"✓ Results saved to ../results/baseline_accuracy.json\")\n",
    "else:\n",
    "    print(\"⚠ Skipping evaluation - no dataset loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: Denormalize images for visualization\n",
    "def denormalize(tensor, mean=IMAGENET_MEAN, std=IMAGENET_STD):\n",
    "    \"\"\"Denormalize image tensor for visualization\"\"\"\n",
    "    tensor = tensor.clone()\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return torch.clamp(tensor, 0, 1)\n",
    "\n",
    "def visualize_predictions(model, dataloader, device, num_samples=6):\n",
    "    \"\"\"Visualize model predictions on sample images\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch\n",
    "    images, labels = next(iter(dataloader))\n",
    "    images = images[:num_samples]\n",
    "    labels = labels[:num_samples]\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images.to(device))\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        top_probs, top_indices = torch.topk(probs, 3)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (img, label, top_p, top_i) in enumerate(zip(images, labels, top_probs, top_indices)):\n",
    "        # Denormalize and convert to numpy\n",
    "        img_display = denormalize(img).permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # Plot image\n",
    "        axes[idx].imshow(img_display)\n",
    "        axes[idx].axis('off')\n",
    "        \n",
    "        # Add title with prediction\n",
    "        true_label = imagenet_labels[label.item()]\n",
    "        pred_label = imagenet_labels[top_i[0].item()]\n",
    "        pred_conf = top_p[0].item()\n",
    "        \n",
    "        color = 'green' if label.item() == top_i[0].item() else 'red'\n",
    "        title = f\"True: {true_label}\\nPred: {pred_label}\\nConf: {pred_conf:.2%}\"\n",
    "        axes[idx].set_title(title, fontsize=10, color=color, weight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/images/baseline_predictions.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"✓ Saved visualization to ../results/images/baseline_predictions.png\")\n",
    "\n",
    "# Visualize if dataset is available\n",
    "if val_loader:\n",
    "    Path(\"../results/images\").mkdir(parents=True, exist_ok=True)\n",
    "    visualize_predictions(model, val_loader, DEVICE)\n",
    "else:\n",
    "    print(\"⚠ Skipping visualization - no dataset loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BASELINE TESTING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n✓ Established baseline accuracy on clean images\")\n",
    "print(\"✓ Identified correctly classified images for adversarial attacks\")\n",
    "print(\"\\n⏭️  NEXT: Implement FGSM adversarial attacks (02_fgsm_attacks.ipynb)\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
